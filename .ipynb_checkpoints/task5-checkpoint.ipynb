{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: the F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up: computing the score for a classification model\n",
    "\n",
    "How do you measure the performance of a classification model such as logistic regression ? An easy answer is to simply take our test set, predict the label for each element in the test set and compare it with the true label. The accuracy score of the model is simply the ratio of correct predictions. For example, if the model predicts correctly 90% of the time, then the accuracy score is `0.9`.\n",
    "\n",
    "To give a better illustration of this we can suppose we trained a model for recognizing cats and dogs on images. Then we tested that model on 200 images (100 cats and 100 dogs) and got the following results:\n",
    "\n",
    "|             | Predicted: cat | Predicted: dog |\n",
    "|-------------|----------------|----------------|\n",
    "| Actual: cat | 91             | 9              |\n",
    "| Actual: dog | 11             | 89             |\n",
    "\n",
    "Among the 100 cats, 91 were correctly classified but 9 were misclassified as dogs. In the same way, among the 100 dogs, 89 were correctly classified as dogs but 11 were misclassified as cats. To compute the accuracy we can simple take all the correctly classified images (`91 + 89 = 180`) and divide this number by the total number of images (`200`) and we get `180 / 200 = 0.9`. So our model's accuracy score is `0.9` !\n",
    "\n",
    "Now let's choose another example: let's suppose we have created a model for predicting whether a person has cancer or not depending on a radio image of her lungs. Let's say we trained our model in a very lazy and dangerous way: we just make it predict that the person doesn't have cancer, no matter the image. That model is really bad anyway. It will probably achieve a score of `0.5` since it is just stupid deterministic guessing. Now let's suppose we have our test data: 200 images, where 5 represent sick lungs. This is what our model achieved:\n",
    "\n",
    "|                   | Predicted: cancer | Predicted: no cancer |\n",
    "|-------------------|-------------------|----------------------|\n",
    "| Actual: cancer    | 0                 | 5                    |\n",
    "| Actual: no cancer | 0                 | 195                  |\n",
    "\n",
    "Now let's compute the score: as before we take the number of correctly classified images (`0 + 195`) and divide it by the number of total images (`200`) and we obtain `195 / 200 = 0.975`. Wait what ?! How is this possible ? There must be a miscalculation somewhere ! This model is even better than the cat/dog classifier !\n",
    "\n",
    "Actually, truth is that accuracy is not the right score metric for this case. In general, in situations where the classes are unbalanced (5 cancer vs 195 non cancer), we should avoid relying on accuracy. But how will we compute the performance of our model then ? Fortunately, there are other metrics available we can rely on.\n",
    "\n",
    "### Recall and Precision\n",
    "\n",
    "**Confusion matrix:** First of all let's generalize the matrices we saw above. In a general yes/no classification task the following matrice is called the **confusion matrix**. \n",
    "\n",
    "|             | Predicted: yes | Predicted: no  |\n",
    "|-------------|----------------|----------------|\n",
    "| Actual: yes | True Positive  | False Negative |\n",
    "| Actual: no  | False Positive | True Negative  |\n",
    "\n",
    "The true positives are the samples that were correctly classified as \"yes\". In the cancer example, there were 0 true positives (TP), 0 false positives (FP), 5 false negatives (FN) and 195 true negatives (TN).\n",
    "\n",
    "We define the **recall score** of the model as the number of true positives divided by the total number of actual \"yes\" samples or:\n",
    "\n",
    "`Recall = TP / (TP + FN)`\n",
    "\n",
    "We also define the **precision score** of the model as the number of true positives divided by the total number of predicted \"yes\" samples or:\n",
    "\n",
    "`Precision = TP / (TP + FP)`\n",
    "\n",
    "The recall for our cancer detection model is `0 / (0 + 5) = 0`. This is it ! While the accuracy score was very high, the recall is `0` and this is telling us that there is something wrong with our model. Think of the recall as the ability of a model to identify all relevant cases, for example to correctly identify cancer regardless of the number of non cancer cases.\n",
    "\n",
    "The precision will be `0 / (0 + 0) = 0 / 0` so there is no point in computing it like this. Let's say the model correctly identified only one case of cancer:\n",
    "\n",
    "|                   | Predicted: cancer | Predicted: no cancer |\n",
    "|-------------------|-------------------|----------------------|\n",
    "| Actual: cancer    | 1                 | 4                    |\n",
    "| Actual: no cancer | 0                 | 195                  |\n",
    "\n",
    "The precision is then `1 / 1 + 0 = 1`. Unfortunately, even though only one case out of 5 was correctly identified as cancer, the model still achieved a precision score of `1.0`. However let us try another bad model: let's suppose that our model has correctly classified all our cancer patients as having cancer, which is good. However, it also classified 15 healthy patients as having cancer.\n",
    "\n",
    "|                   | Predicted: cancer | Predicted: no cancer |\n",
    "|-------------------|-------------------|----------------------|\n",
    "| Actual: cancer    | 5                 | 0                    |\n",
    "| Actual: no cancer | 15                | 180                  |\n",
    "\n",
    "The accuracy is `(5 + 180) / 200 = 0.925` and the recall is `5 / (5 + 0) = 1.0`. However, the precision is `5 / (5 + 15) = 0.25` ! Think of precision as the ability of the model to return only relevant instances.\n",
    "\n",
    "There are situations where we do not mind if the recall score is lower and there are others where the precision score is not that important. Choosing the right score metric is not always straightforward and requires careful thinking about the nature of the task at hand.\n",
    "\n",
    "In general, if we want a good model for all situations, we want to verify that precision and recall scores are both high.\n",
    "\n",
    "### The F1 score\n",
    "Ideally, we would like to be sure that our model has a good score in both precision and recall aspect. For this we can simply compute the arithmetic mean of the two values: `precision + recall / 2`. So if one of the two is 0 and the other is 1, then the mean will be `0.5`. However, we would like to go further and completely penalize low precision or recall. Ideally, if one of the 2 is `0` then we would like the final score to be `0`. For this we can use the **F1 score** which is simply the harmonic mean of the precision and the recall:\n",
    "\n",
    "`F1 score = 2 * (recall * precision) / (recall + precision)`\n",
    "\n",
    "Now we can try: if one of the two is `0` and the other is `1` then the final score is `2 * (1 * 0) / (1 + 0) = 0`. This way we can protect ourself from falling into the \"accuracy trap\".\n",
    "\n",
    "### Scikit learn example\n",
    "\n",
    "We will now show an example in Python with the sklearn library. We will have two classes where the samples are generated according to a multivariate Gaussian distribution. The first class will be generated with mean `2` and the second class with mean `1.3` (Comment: the same dataset as in task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_samples(nb_features = 20, mean0 = 2, mean1 = 1.3, nb_samples0 = 2000, nb_samples1 = 2000):\n",
    "    mean0_array = [mean0] * nb_features\n",
    "    mean1_array = [mean1] * nb_features\n",
    "    cov = np.identity(nb_features)\n",
    "    samples0_features = np.random.multivariate_normal(mean0_array, cov, nb_samples0)\n",
    "    samples1_features = np.random.multivariate_normal(mean1_array, cov, nb_samples1)\n",
    "    \n",
    "    labels0 = [0] * nb_samples0\n",
    "    labels1 = [1] * nb_samples1\n",
    "    return samples0_features, samples1_features, labels0, labels1\n",
    "\n",
    "def get_train_test(nb_features = 20, mean0 = 2, mean1 = 1.3, nb_samples0 = 2000, nb_samples1 = 2000):\n",
    "    features0, features1, labels0, labels1 = get_samples(nb_features, mean0, mean1, nb_samples0, nb_samples1)\n",
    "    features0_train, features0_test, labels0_train, labels0_test = train_test_split(features0, labels0, test_size = 0.3)\n",
    "    features1_train, features1_test, labels1_train, labels1_test = train_test_split(features1, labels1, test_size = 0.3)\n",
    "    \n",
    "    X_train = np.append(features0_train, features1_train, axis=0)\n",
    "    X_test = np.append(features0_test, features1_test, axis=0)\n",
    "    y_train = labels0_train + labels1_train\n",
    "    y_test = labels0_test + labels1_test\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have the balanced dataset where both classes have 2000 samples. We use simple logistic regression without any parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the three score values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is 0.591\n",
      "Precision score is 0.433\n",
      "Recall score is 0.929\n"
     ]
    }
   ],
   "source": [
    "predictions = lg.predict(X_test)\n",
    "f1 = f1_score(predictions, y_test)\n",
    "precision = precision_score(predictions, y_test)\n",
    "recall = recall_score(predictions, y_test)\n",
    "print(\"F1 score is {0:.3f}\".format(f1))\n",
    "print(\"Precision score is {0:.3f}\".format(precision))\n",
    "print(\"Recall score is {0:.3f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three score are quite good, which was expected since the dataset was balanced. Now let's try with an unbalanced dataset where class 0 has 2000 samples but class 1 has only 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test(nb_samples1=100)\n",
    "lg = LogisticRegression()\n",
    "lg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score is 0.591\n",
      "Precision score is 0.433\n",
      "Recall score is 0.929\n"
     ]
    }
   ],
   "source": [
    "predictions = lg.predict(X_test)\n",
    "f1 = f1_score(predictions, y_test)\n",
    "precision = precision_score(predictions, y_test)\n",
    "recall = recall_score(predictions, y_test)\n",
    "print(\"F1 score is {0:.3f}\".format(f1))\n",
    "print(\"Precision score is {0:.3f}\".format(precision))\n",
    "print(\"Recall score is {0:.3f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we obtained a good recall but a very low precision score (and hence a low F1 score), which was expected since there were only 100 samples in the second class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
